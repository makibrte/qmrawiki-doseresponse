{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Output for Exp. 183\n",
      "['Dose', 'Infected', 'Non-infected', 'Total']\n",
      "   Dose Infected Non-infected Total\n",
      "0   100        2            2     4\n",
      "1   300        2            3     5\n",
      "2  1000        1            2     3\n",
      "3  3000        3            1     4\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# The target URL\n",
    "url = 'https://qmrawiki.org/node/402'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find elements by HTML tags, attributes, or CSS class\n",
    "    titles = soup.find_all('h1')\n",
    "    for title in titles:\n",
    "        print(title.get_text())\n",
    "    tables_data = soup.find_all('table')\n",
    "    \n",
    "    for table_data in tables_data[3]:\n",
    "        table_row_data = [row for row in table_data.get_text().split('\\n') if row != '']\n",
    "        if len(table_row_data):\n",
    "            columns = table_row_data[1:5]\n",
    "            experiment_data = []\n",
    "            print(columns)\n",
    "            i = 5\n",
    "            while i <= len(table_row_data):\n",
    "                experiment_data\n",
    "                row_data = table_row_data[i:i+4]\n",
    "                i+=4 \n",
    "                if(len(row_data) > 0):\n",
    "                    experiment_data.append(row_data)\n",
    "    df = pd.DataFrame(columns=columns, data=experiment_data)\n",
    "    print(df)         \n",
    "else:\n",
    "    print('Failed to retrieve the webpage')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n108', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oral', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '8', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Iowa', 'strain', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oocysts', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'human', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'exponential', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n\\nk', '=', '4.19E-03\\nLD50/ID50', '=', '1.65E+02', '\\n', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'infection', '', '', '', '', '', '', '', '', '', '', '', '\\n\\nDuPont,', 'H', 'L.,', 'et', 'al.', '\"The', 'infectivity', 'of', 'Cryptosporidium', 'parvum', 'in', 'healthy', 'volunteers.\"', 'The', 'New', 'England', 'journal', 'of', 'medicine.', '332', '(1995):', '13.', '\\n']\n",
      "['\\n\\n139', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oral', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '8', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Iowa', 'isolate', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oocysts', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'human', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'exponential', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n\\nk', '=', '5.26E-03\\nLD50/ID50', '=', '1.32E+02', '\\n', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'infection', '', '', '', '', '', '', '', '', '', '', '', '\\n\\nMessner,', 'M', 'J.,', 'C', 'L.', 'Chappell,', 'and', 'P', 'C.', 'Okhuysen.', '\"Risk', 'Assessment', 'for', 'Cryptosporidium:', 'A', 'Hierarchical', 'Bayesian', 'Analysis', 'of', 'Human', 'Dose', 'Response', 'Data.\"', 'Water', 'Research.', '35', '(2001):', '16.', '\\n']\n",
      "['\\n\\n140', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oral', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '4', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'TAMU', 'isolate', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oocysts', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'human', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'exponential', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n\\nk', '=', '5.72E-02\\nLD50/ID50', '=', '1.21E+01', '\\n', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'infection', '', '', '', '', '', '', '', '', '', '', '', '\\n\\nMessner,', 'M', 'J.,', 'C', 'L.', 'Chappell,', 'and', 'P', 'C.', 'Okhuysen.', '\"Risk', 'Assessment', 'for', 'Cryptosporidium:', 'A', 'Hierarchical', 'Bayesian', 'Analysis', 'of', 'Human', 'Dose', 'Response', 'Data.\"', 'Water', 'Research.', '35', '(2001):', '16.', '\\n']\n",
      "['\\n\\n141', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oral', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '4', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'UCP', 'isolate', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oocysts', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'human', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'beta-Poisson', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'a', '=', '1.45E-01\\n\\nLD50/ID50', '=', '1.79E+02', '\\nN50', '=', '1.79E+02', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'infection', '', '', '', '', '', '', '', '', '', '', '', '\\n\\nCoster,', 'T', 'S.,', 'et', 'al.', '\"Immune', 'response,', 'ciprofloxacin', 'activity,', 'and', 'gender', 'differences', 'after', 'human', 'experimental', 'challenge', 'by', 'two', 'strains', 'of', 'enterotoxigenic', 'Escherichia', 'coli.\"', 'Infection', 'and', 'immunity.', '75', '(2007):', '1.', '\\n']\n",
      "['\\n\\n181', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oral', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '4', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '*C.', 'hominis*,', 'TU502', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oocysts', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'human', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'beta-Poisson', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'a', '=', '2.7E-01\\n\\nLD50/ID50', '=', '1.68E+01', '\\nN50', '=', '1.68E+01', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'diarrhea', '', '', '', '', '', '', '', '', '', '', '', '\\n\\nOkhuysen,', 'P', 'C.,', 'et', 'al.', '\"Infectivity', 'of', 'a', 'Cryptosporidium', 'parvum', 'Isolate', 'of', 'Cervine', 'Origin', 'for', 'Healthy', 'Adults', 'and', 'Interferon-Î³', 'Knockout', 'Mice.\"', 'Journal', 'of', 'Infectious', 'Diseases.', '185', '(2002):', '9.', '\\n']\n",
      "['\\n\\n183', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oral', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '4', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Moredun', 'isolate', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'oocysts', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'human', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'beta-Poisson', '', '', '', '', '', '', '', '', '', '', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'a', '=', '1.14E-01\\n\\nLD50/ID50', '=', '4.55E+02', '\\nN50', '=', '4.55E+02', '', '\\n\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', 'infection', '', '', '', '', '', '', '', '', '', '', '', '\\n\\nBlaser,', 'M', 'J.,', 'et', 'al.', '\"Experimental', 'Campylobacter', 'jejuni', 'Infection', 'of', 'Adult', 'Mice.\"', 'Infection', 'and', 'Immunity.', '39', '(1983):', '2.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "url_new = 'https://qmrawiki.org/experiments/cryptosporidium-parvum'\n",
    "\n",
    "response = requests.get(url_new)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    experiments_container = soup.find(class_='view-pathogen-dose-response-experiments')\n",
    "    \n",
    "    #experiments = experiments_container.find_all(class_='response-table-inner')\n",
    "\n",
    "    #for experiment in experiments:\n",
    "        #print(experiment.get_text().split(' '))\n",
    "        \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1, iterating over all the dose response pages and going to next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dose_responses_url = 'https://qmrawiki.org/framework/dose-response/experiments'\n",
    "response = requests.get(dose_responses_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dose response has inline dataset\n",
      "  Dose (no. of organisms) Positive Responses Negative Responses  \\\n",
      "2                      10                  0                  8   \n",
      "0                     100                  0                 16   \n",
      "3                    1000                  2                 16   \n",
      "1                   10000                  5                  3   \n",
      "\n",
      "  Total Subjects/Responses  \n",
      "2                        8  \n",
      "0                       16  \n",
      "3                       18  \n",
      "1                        8  \n"
     ]
    }
   ],
   "source": [
    "individual_dose_responses_list = soup.find(class_='quicktabs-views-group')\n",
    "\n",
    "dose_response_url = individual_dose_responses_list.find_all('span')\n",
    "base_url = 'https://qmrawiki.org'\n",
    "for dose_responses in dose_response_url:\n",
    "    temps = dose_responses.find_all('a')\n",
    "    for temp in temps:\n",
    "        experiment_overview_url = temp.get('href')\n",
    "        extract_from_dose_response_page(base_url+experiment_overview_url, base_url)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_params_parser(params_list):\n",
    "    params = []\n",
    "    values = []\n",
    "    for param in params_list:\n",
    "        _ = param.split(' ')\n",
    "        params.append(_[0])\n",
    "        values.append(float(_[-1].replace(',', '')))\n",
    "    return [params, values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(soup, experiment_id):\n",
    "        \n",
    "        \n",
    "        experiment_exposure_route = soup.select('td.views-field-field-exposure-route')\n",
    "        for route in experiment_exposure_route:\n",
    "            exposure_route = route.get_text().strip()\n",
    "        experiment_strain_container = soup.select('td.views-field-field-agent-strain')\n",
    "        for strain in experiment_strain_container:\n",
    "            experiment_strain = strain.get_text().strip()\n",
    "        experiment_doseunits_container = soup.select('td.views-field-field-dose-units')\n",
    "        for dose in experiment_doseunits_container:\n",
    "            experiment_dose_units = dose.get_text().strip()\n",
    "        experiment_host_container = soup.select('td.views-field-field-host-type')\n",
    "        for host in experiment_host_container:\n",
    "            experiment_host = host.get_text().strip()\n",
    "        experiment_modeltype_container = soup.select('td.views-field-field-best-fit-model')\n",
    "        for modeltype in experiment_modeltype_container:\n",
    "            experiment_model = modeltype.get_text().strip()\n",
    "        #TODO: Create this into a separate function\n",
    "        experiment_params_container = soup.select('td.views-field-nothing')\n",
    "        for param in experiment_params_container:\n",
    "            experiment_optimal_params, experiment_optim_vals = optimal_params_parser(param.get_text().strip().split('\\n'))\n",
    "        ########\n",
    "        experiment_response_cont = soup.select('td.views-field-field-response')\n",
    "        for response in experiment_response_cont:\n",
    "            experiment_response_type = response.get_text().strip()\n",
    "\n",
    "        metadata_cols = ['Experiment Id', 'Exposure Route', 'Agent Strain', 'Dose Units', 'Host', 'Best Fit Model', f'Optimized Params {' '.join(experiment_optimal_params)}', \n",
    "                            'Response Type']\n",
    "        metadata_data = [experiment_id, exposure_route, experiment_strain, experiment_dose_units, experiment_host,\n",
    "                        experiment_model, experiment_optim_vals, experiment_response_type]\n",
    "            \n",
    "        metadata_df = pd.DataFrame([metadata_data], columns=metadata_cols)\n",
    "        metadata_df.to_csv(f'{experiment_id}_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Have to adjust for pages that have the experiment data inside Experiment Dataset\n",
    "def extract_from_dose_response_page(url, base_url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #Getting experiment table data\n",
    "        experimentId_container = soup.select('td.views-field-field-experiment-id-text')\n",
    "        \n",
    "        for container in experimentId_container:\n",
    "            experiment_url = container.find('a').get('href')\n",
    "            experiment_id = container.find('a').get_text()            \n",
    "            extract_from_experiment_page(base_url + experiment_url)\n",
    "        \n",
    "        #Getting experiment META data\n",
    "        extract_metadata(soup, experiment_id)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_experiment_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        experiment_dataset = soup.find_all(id='tablefield-0')\n",
    "\n",
    "        for data in experiment_dataset:\n",
    "            experiment_data_table = data.find_all('tr')\n",
    "            if(len(experiment_data_table) > 0):\n",
    "                print('Dose response has inline dataset')\n",
    "                df = parse_meta_table(experiment_data_table, response.text)\n",
    "                #TODO: Add function here to extract the data if the dataset is embeded\n",
    "            else:\n",
    "                experiment_data_html = extract_html_table(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_meta_table(data, page_txt):\n",
    "    i = 0\n",
    "    soup = BeautifulSoup(page_txt, 'html.parser')\n",
    "    odds = soup.select('tr.odd')    \n",
    "    evens = soup.select('tr.even')\n",
    "    columns = soup.select('th.row_0')\n",
    "    column_names = [column.get_text() for column in columns]\n",
    "    rows = []\n",
    "    even_counter = 2\n",
    "    odd_counter = 1\n",
    "    for even in evens:\n",
    "        \n",
    "        row = []\n",
    "        for n in even.find_all(class_=f'row_{even_counter}'):\n",
    "    \n",
    "            row.append(n.get_text())\n",
    "        even_counter += 2\n",
    "        rows.append(row)\n",
    "    for odd in odds:\n",
    "        row = []\n",
    "        \n",
    "        for n in odd.find_all(class_=f'row_{odd_counter}'):\n",
    "            \n",
    "            row.append(n.get_text())\n",
    "        rows.append(row)     \n",
    "        odd_counter += 2\n",
    "    df = pd.DataFrame(columns=column_names, data=rows) \n",
    "    return df.sort_values(['Dose (no. of organisms)'])\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikiparser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
